#!/usr/bin/env python3
"""
Kigurumi Market Intelligence Monitor - Phase 2
æŒç»­ç¤¾åŒºç›‘æµ‹è¿è¥ç³»ç»Ÿ

åŠŸèƒ½ï¼š
1. Telegram ç¤¾åŒºæ•°æ®é‡‡é›†ï¼ˆä½¿ç”¨ OpenClaw message APIï¼‰
2. Web æœç´¢ç«å“è¿½è¸ª
3. ç”¨æˆ·ç”»åƒåˆ†æ
4. æ¯æ—¥æŠ¥å‘Šç”Ÿæˆ

Author: AI Agent
Phase: 2 (Continuous Monitoring)
"""

import json
import os
import re
import time
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional, Set, Any
from dataclasses import dataclass, asdict
from collections import Counter, defaultdict
import logging
import subprocess

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class MonitoredMessage:
    """ç›‘æµ‹åˆ°çš„æ¶ˆæ¯æ•°æ®ç»“æ„"""
    id: str
    source: str
    source_type: str  # channel / group / search
    author: str
    content: str
    timestamp: str
    keywords_found: List[str]
    hashtags: List[str]
    urls: List[str]
    media_type: Optional[str]
    sentiment: Optional[str]  # positive / neutral / negative
    message_type: str  # discussion / sale / event / review / other
    collected_at: str
    
    def to_dict(self) -> Dict:
        return asdict(self)


@dataclass
class CompetitorIntel:
    """ç«å“æƒ…æŠ¥æ•°æ®ç»“æ„"""
    name: str
    source: str
    mention_date: str
    context: str
    product_type: Optional[str]
    price_range: Optional[str]
    sentiment: str
    urls: List[str]
    collected_at: str
    
    def to_dict(self) -> Dict:
        return asdict(self)


class KigurumiPhase2Monitor:
    """Kigurumi Phase 2 æŒç»­ç›‘æµ‹å™¨"""
    
    # ç›‘æµ‹å…³é”®è¯é…ç½® - æ‰©å±•ç‰ˆ
    KEYWORDS = {
        'primary': [
            'kigurumi', 'ç€ãã‚‹ã¿', 'ã‚­ã‚°ãƒ«ãƒŸ', 
            'kig', 'å¤´å£³', 'kiger', 'ç€ãã‚‹ã¿ã•ã‚“'
        ],
        'secondary': [
            'animegao', 'ã‚¢ãƒ‹ãƒ¡é¡”', 'mask', 'é¢å…·',
            'hadalabo', 'è‚Œãƒ©ãƒœ', 'bodysuit', 'ç´§èº«è¡£'
        ],
        'product': [
            'å¤´å£³å‡ºå”®', 'kigurumi sale', 'ç€ãã‚‹ã¿ è²©å£²',
            'mask for sale', 'commission', 'å§”æ‰˜', 'å®šåˆ¶',
            'äºŒæ‰‹', 'è½¬è®©', 'æ±‚è´­', 'buy', 'sell', 'trade'
        ],
        'event': [
            'event', 'æ´»åŠ¨', 'å±•ä¼š', 'convention',
            'meetup', 'èšä¼š', 'cf', 'comiket', 'æ¼«å±•',
            'cosplay event', 'kigurumi meet'
        ],
        'brands': [
            'dollkii', 'nfd', 'niya', 'kigmask',
            'kigurumi-online', 'animegao mall',
            'damegami', 'kigland', 'kigdom',
            'hiyasuya', 'é­”å¯¼', 'kigstudio'
        ]
    }
    
    # å·²çŸ¥ Kigurumi å“ç‰Œ/å·¥ä½œå®¤ï¼ˆç«å“ï¼‰
    COMPETITORS = {
        'å¤´å£³åˆ¶ä½œå·¥ä½œå®¤': [
            'Dollkii', 'NFD Studio', 'Niya Kigurumi', 
            'KigMask', 'KigLand', 'KigDom', 'Hiyasuya',
            'é­”å¯¼å…·å·¥ä½œå®¤', 'KigStudio', 'AniMask'
        ],
        'æœè£…/é…ä»¶': [
            'Hadalabo', 'è‚Œãƒ©ãƒœ', 'Kigurumi-Online',
            'Animegao Mall', 'Damegami', 'Kigurumi Shop'
        ],
        'ç»¼åˆå¹³å°': [
            'Booth.pm', 'Twitter/X Kigurumi', 
            'Pixiv ç€ãã‚‹ã¿', 'Reddit r/Kigurumi'
        ]
    }
    
    # Telegram ç›‘æµ‹æºé…ç½®
    SOURCES = {
        'channels': [
            {'name': 'Kigurumi World', 'id': '@kigurumi_world', 'language': 'en', 'active': True},
            {'name': 'Animegao Kigurumi', 'id': '@animegao_kigurumi', 'language': 'en', 'active': True},
            {'name': 'ç€ãã‚‹ã¿æƒ…å ±å±€', 'id': '@kigurumi_jp_info', 'language': 'jp', 'active': True},
            {'name': 'Kigurumi ä¸­æ–‡åœˆ', 'id': '@kigurumi_cn', 'language': 'zh', 'active': True},
            {'name': 'Kigurumi Sale', 'id': '@kigurumi_sale', 'language': 'en', 'active': False},
        ],
        'groups': [
            {'name': 'Kigurumi Fan Club', 'id': '@kigurumifanclub', 'language': 'en', 'active': True},
            {'name': 'ç€ãã‚‹ã¿å¥½ã', 'id': '@kigurumi_suki', 'language': 'jp', 'active': True},
            {'name': 'KIG å¤´å£³äº¤æµ', 'id': '@kig_head_exchange', 'language': 'zh', 'active': True},
            {'name': 'Kigurumi Buy/Sell', 'id': '@kigurumi_trade', 'language': 'en', 'active': False},
        ]
    }
    
    def __init__(self, base_dir: str = "research/kigurumi"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(parents=True, exist_ok=True)
        
        # å­ç›®å½•
        self.data_dir = self.base_dir / "community-data"
        self.reports_dir = self.base_dir / "daily-reports"
        self.data_dir.mkdir(exist_ok=True)
        self.reports_dir.mkdir(exist_ok=True)
        
        # å­˜å‚¨æ–‡ä»¶
        self.messages_file = self.data_dir / "messages.jsonl"
        self.competitor_file = self.data_dir / "competitor_intel.jsonl"
        self.stats_file = self.data_dir / "stats.json"
        self.seen_ids_file = self.data_dir / "seen_ids.json"
        
        # å·²å¤„ç†çš„æ¶ˆæ¯ID
        self.seen_ids: Set[str] = self._load_seen_ids()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = self._load_stats()
        
        # ç”¨æˆ·ç”»åƒæ•°æ®
        self.user_profiles: Dict[str, Dict] = {}
        
        logger.info(f"Phase 2 Monitor initialized")
        logger.info(f"Base dir: {self.base_dir}")
        logger.info(f"Seen messages: {len(self.seen_ids)}")
    
    def _load_seen_ids(self) -> Set[str]:
        """åŠ è½½å·²å¤„ç†çš„æ¶ˆæ¯ID"""
        if self.seen_ids_file.exists():
            with open(self.seen_ids_file, 'r', encoding='utf-8') as f:
                return set(json.load(f))
        return set()
    
    def _save_seen_ids(self):
        """ä¿å­˜å·²å¤„ç†çš„æ¶ˆæ¯ID"""
        with open(self.seen_ids_file, 'w', encoding='utf-8') as f:
            json.dump(list(self.seen_ids), f, ensure_ascii=False)
    
    def _load_stats(self) -> Dict:
        """åŠ è½½ç»Ÿè®¡æ•°æ®"""
        if self.stats_file.exists():
            with open(self.stats_file, 'r', encoding='utf-8') as f:
                stats = json.load(f)
                # ç¡®ä¿ Phase 2 çš„æ–°å­—æ®µå­˜åœ¨
                if 'by_message_type' not in stats:
                    stats['by_message_type'] = {}
                return stats
        return {
            'total_collected': 0,
            'by_source': {},
            'by_keyword': {},
            'by_date': {},
            'by_message_type': {},
            'last_run': None
        }
    
    def _save_stats(self):
        """ä¿å­˜ç»Ÿè®¡æ•°æ®"""
        with open(self.stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)
    
    def extract_keywords(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–åŒ¹é…çš„å…³é”®è¯"""
        text_lower = text.lower()
        found = []
        
        for category, keywords in self.KEYWORDS.items():
            for kw in keywords:
                if kw.lower() in text_lower:
                    found.append(kw)
        
        return list(set(found))
    
    def extract_hashtags(self, text: str) -> List[str]:
        """æå–è¯é¢˜æ ‡ç­¾"""
        return re.findall(r'#[\w\u4e00-\u9fff]+', text)
    
    def extract_urls(self, text: str) -> List[str]:
        """æå–URLé“¾æ¥"""
        url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
        return re.findall(url_pattern, text)
    
    def detect_message_type(self, text: str, hashtags: List[str]) -> str:
        """æ£€æµ‹æ¶ˆæ¯ç±»å‹"""
        text_lower = text.lower()
        
        # äº¤æ˜“ç›¸å…³
        sale_keywords = ['å‡ºå”®', 'è½¬è®©', 'sale', 'buy', 'æ±‚è´­', 'äºŒæ‰‹', 'price', 'ä»·æ ¼', 'é¢„ç®—', 'è²©å£²']
        if any(kw in text_lower for kw in sale_keywords):
            return 'sale'
        
        # æ´»åŠ¨ç›¸å…³
        event_keywords = ['event', 'æ´»åŠ¨', 'å±•ä¼š', 'convention', 'meetup', 'èšä¼š', 'comiket', 'æ¼«å±•']
        if any(kw in text_lower for kw in event_keywords):
            return 'event'
        
        # è¯„æµ‹/åˆ†äº«
        review_keywords = ['review', 'è¯„æµ‹', 'æµ‹è¯„', 'ä½“éªŒ', 'å¿ƒå¾—', 'æ¨è']
        if any(kw in text_lower for kw in review_keywords):
            return 'review'
        
        # æŠ€æœ¯/åˆ¶ä½œ
        tech_keywords = ['åˆ¶ä½œ', 'diy', 'æ•™ç¨‹', 'æ”¹é€ ', 'å–·æ¼†', 'åŒ–å¦†', 'ä¿®å¤']
        if any(kw in text_lower for kw in tech_keywords):
            return 'technical'
        
        return 'discussion'
    
    def analyze_sentiment(self, text: str) -> str:
        """ç®€å•æƒ…æ„Ÿåˆ†æ"""
        positive_words = ['å–œæ¬¢', 'love', 'amazing', 'great', 'awesome', 'beautiful', 'cute', 
                         'å¯çˆ±', 'èµ', 'æ£’', 'perfect', 'wonderful', 'æ„Ÿè°¢', 'è°¢è°¢']
        negative_words = ['è®¨åŒ', 'hate', 'terrible', 'bad', 'awful', 'problem', 'issue',
                         'å¤±æœ›', 'å·®', 'è´µ', 'å‘', 'scam', 'fraud', 'éª—']
        
        text_lower = text.lower()
        pos_count = sum(1 for w in positive_words if w in text_lower)
        neg_count = sum(1 for w in negative_words if w in text_lower)
        
        if pos_count > neg_count:
            return 'positive'
        elif neg_count > pos_count:
            return 'negative'
        return 'neutral'
    
    def detect_competitors(self, text: str) -> List[Dict]:
        """æ£€æµ‹ç«å“æåŠ"""
        mentions = []
        text_lower = text.lower()
        
        for category, brands in self.COMPETITORS.items():
            for brand in brands:
                if brand.lower() in text_lower:
                    # æå–ä¸Šä¸‹æ–‡
                    idx = text_lower.find(brand.lower())
                    start = max(0, idx - 50)
                    end = min(len(text), idx + len(brand) + 50)
                    context = text[start:end]
                    
                    mentions.append({
                        'brand': brand,
                        'category': category,
                        'context': context,
                        'sentiment': self.analyze_sentiment(text)
                    })
        
        return mentions
    
    def generate_message_id(self, source: str, content: str, timestamp: str) -> str:
        """ç”Ÿæˆå”¯ä¸€æ¶ˆæ¯ID"""
        data = f"{source}:{content[:100]}:{timestamp}"
        return hashlib.md5(data.encode()).hexdigest()
    
    def process_message(self, source: str, source_type: str, 
                       author: str, content: str, 
                       timestamp: str, media_type: Optional[str] = None) -> Optional[MonitoredMessage]:
        """å¤„ç†å•æ¡æ¶ˆæ¯"""
        
        msg_id = self.generate_message_id(source, content, timestamp)
        
        if msg_id in self.seen_ids:
            return None
        
        keywords_found = self.extract_keywords(content)
        hashtags = self.extract_hashtags(content)
        urls = self.extract_urls(content)
        message_type = self.detect_message_type(content, hashtags)
        sentiment = self.analyze_sentiment(content)
        
        msg = MonitoredMessage(
            id=msg_id,
            source=source,
            source_type=source_type,
            author=author,
            content=content,
            timestamp=timestamp,
            keywords_found=keywords_found,
            hashtags=hashtags,
            urls=urls,
            media_type=media_type,
            sentiment=sentiment,
            message_type=message_type,
            collected_at=datetime.now().isoformat()
        )
        
        return msg
    
    def save_message(self, msg: MonitoredMessage):
        """ä¿å­˜æ¶ˆæ¯åˆ°æ–‡ä»¶"""
        with open(self.messages_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(msg.to_dict(), ensure_ascii=False) + '\n')
        
        self.seen_ids.add(msg.id)
        self._update_stats(msg)
    
    def save_competitor_intel(self, intel: CompetitorIntel):
        """ä¿å­˜ç«å“æƒ…æŠ¥"""
        with open(self.competitor_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(intel.to_dict(), ensure_ascii=False) + '\n')
    
    def _update_stats(self, msg: MonitoredMessage):
        """æ›´æ–°ç»Ÿè®¡æ•°æ®"""
        self.stats['total_collected'] += 1
        
        source = msg.source
        self.stats['by_source'][source] = self.stats['by_source'].get(source, 0) + 1
        
        for kw in msg.keywords_found:
            self.stats['by_keyword'][kw] = self.stats['by_keyword'].get(kw, 0) + 1
        
        date = msg.collected_at[:10]
        self.stats['by_date'][date] = self.stats['by_date'].get(date, 0) + 1
        
        msg_type = msg.message_type
        self.stats['by_message_type'][msg_type] = self.stats['by_message_type'].get(msg_type, 0) + 1
        
        self.stats['last_run'] = datetime.now().isoformat()
    
    def load_messages(self, days: int = 30) -> List[Dict]:
        """åŠ è½½æŒ‡å®šå¤©æ•°å†…çš„æ¶ˆæ¯"""
        cutoff = (datetime.now() - timedelta(days=days)).isoformat()
        messages = []
        
        if self.messages_file.exists():
            with open(self.messages_file, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        msg = json.loads(line.strip())
                        if msg['collected_at'] > cutoff:
                            messages.append(msg)
                    except:
                        continue
        
        return messages
    
    def analyze_user_personas(self, messages: List[Dict]) -> Dict[str, Any]:
        """åˆ†æç”¨æˆ·ç”»åƒ"""
        user_data = defaultdict(lambda: {
            'message_count': 0,
            'sources': set(),
            'keywords': Counter(),
            'message_types': Counter(),
            'languages': Counter(),
            'avg_message_length': 0,
            'total_length': 0
        })
        
        for msg in messages:
            author = msg.get('author', 'unknown')
            user_data[author]['message_count'] += 1
            user_data[author]['sources'].add(msg.get('source', 'unknown'))
            user_data[author]['keywords'].update(msg.get('keywords_found', []))
            user_data[author]['message_types'].update([msg.get('message_type', 'unknown')])
            
            content = msg.get('content', '')
            user_data[author]['total_length'] += len(content)
            
            # ç®€å•è¯­è¨€æ£€æµ‹
            if any('\u4e00' <= c <= '\u9fff' for c in content):
                user_data[author]['languages']['zh'] += 1
            elif any('\u3040' <= c <= '\u309f' or '\u30a0' <= c <= '\u30ff' for c in content):
                user_data[author]['languages']['jp'] += 1
            else:
                user_data[author]['languages']['en'] += 1
        
        # è®¡ç®—å¹³å‡å€¼å¹¶è½¬æ¢é›†åˆ
        for author, data in user_data.items():
            if data['message_count'] > 0:
                data['avg_message_length'] = data['total_length'] // data['message_count']
            data['sources'] = list(data['sources'])
        
        # åˆ†ç±»ç”¨æˆ·ç±»å‹
        personas = {
            'enthusiast': [],  # çˆ±å¥½è€… - æ´»è·ƒè®¨è®º
            'buyer': [],       # ä¹°å®¶ - æ±‚è´­/å’¨è¯¢
            'seller': [],      # å–å®¶ - å‡ºå”®/æ¨å¹¿
            'creator': [],     # åˆ›ä½œè€… - åˆ¶ä½œåˆ†äº«
            'lurker': []       # æ½œæ°´è€… - ä½æ´»è·ƒåº¦
        }
        
        for author, data in user_data.items():
            if data['message_count'] >= 5:
                if 'sale' in data['message_types']:
                    personas['seller'].append(author)
                elif data['keywords']['å¤´å£³'] + data['keywords']['kigurumi'] > 5:
                    personas['enthusiast'].append(author)
                else:
                    personas['creator'].append(author)
            elif data['message_count'] >= 2:
                personas['buyer'].append(author)
            else:
                personas['lurker'].append(author)
        
        return {
            'user_details': dict(user_data),
            'personas': personas,
            'total_users': len(user_data),
            'active_users': len([u for u in user_data.values() if u['message_count'] >= 3])
        }
    
    def analyze_market_trends(self, messages: List[Dict]) -> Dict[str, Any]:
        """åˆ†æå¸‚åœºè¶‹åŠ¿"""
        # æŒ‰æ—¥æœŸèšåˆ
        daily_data = defaultdict(lambda: {
            'count': 0,
            'keywords': Counter(),
            'types': Counter(),
            'sentiment': Counter()
        })
        
        for msg in messages:
            date = msg.get('collected_at', '')[:10]
            daily_data[date]['count'] += 1
            daily_data[date]['keywords'].update(msg.get('keywords_found', []))
            daily_data[date]['types'].update([msg.get('message_type', 'unknown')])
            daily_data[date]['sentiment'].update([msg.get('sentiment', 'neutral')])
        
        # çƒ­é—¨è¯é¢˜
        all_keywords = Counter()
        for msg in messages:
            all_keywords.update(msg.get('keywords_found', []))
        
        # äº¤æ˜“è¶‹åŠ¿
        sale_messages = [m for m in messages if m.get('message_type') == 'sale']
        
        return {
            'daily_activity': dict(daily_data),
            'top_keywords': dict(all_keywords.most_common(20)),
            'message_type_distribution': dict(Counter(m.get('message_type', 'unknown') for m in messages)),
            'sentiment_distribution': dict(Counter(m.get('sentiment', 'neutral') for m in messages)),
            'sale_activity': len(sale_messages),
            'event_mentions': len([m for m in messages if m.get('message_type') == 'event'])
        }
    
    def generate_daily_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ¯æ—¥ç›‘æµ‹æŠ¥å‘Š"""
        today = datetime.now().strftime('%Y-%m-%d')
        yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        
        # åŠ è½½ä»Šæ—¥å’Œè¿‘æœŸæ•°æ®
        today_messages = [m for m in self.load_messages(days=1) 
                         if m.get('collected_at', '').startswith(today)]
        week_messages = self.load_messages(days=7)
        
        # ç”¨æˆ·ç”»åƒåˆ†æ
        user_analysis = self.analyze_user_personas(week_messages)
        
        # å¸‚åœºè¶‹åŠ¿åˆ†æ
        market_trends = self.analyze_market_trends(week_messages)
        
        report = {
            'report_date': today,
            'generated_at': datetime.now().isoformat(),
            'period': 'daily',
            'summary': {
                'new_messages_today': len(today_messages),
                'total_messages_week': len(week_messages),
                'active_sources': len(set(m.get('source') for m in today_messages)),
                'active_users': len(set(m.get('author') for m in today_messages))
            },
            'activity': {
                'by_hour': self._aggregate_by_hour(today_messages),
                'by_source': dict(Counter(m.get('source') for m in today_messages)),
                'by_type': dict(Counter(m.get('message_type') for m in today_messages))
            },
            'content_analysis': {
                'top_keywords': market_trends['top_keywords'],
                'trending_hashtags': self._extract_trending_hashtags(today_messages),
                'hot_topics': self._identify_hot_topics(today_messages)
            },
            'user_personas': user_analysis['personas'],
            'market_trends': {
                'sentiment': market_trends['sentiment_distribution'],
                'sale_activity': market_trends['sale_activity'],
                'event_mentions': market_trends['event_mentions']
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.reports_dir / f"report_{today}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # åŒæ—¶ç”Ÿæˆ Markdown ç‰ˆæœ¬ä¾¿äºé˜…è¯»
        self._generate_markdown_report(report, today)
        
        logger.info(f"Daily report generated: {report_file}")
        return report
    
    def _aggregate_by_hour(self, messages: List[Dict]) -> Dict[str, int]:
        """æŒ‰å°æ—¶èšåˆæ¶ˆæ¯"""
        hours = defaultdict(int)
        for msg in messages:
            try:
                hour = msg.get('collected_at', '')[11:13]
                if hour:
                    hours[hour] += 1
            except:
                continue
        return dict(hours)
    
    def _extract_trending_hashtags(self, messages: List[Dict]) -> List[Dict]:
        """æå–çƒ­é—¨æ ‡ç­¾"""
        hashtags = Counter()
        for msg in messages:
            hashtags.update(msg.get('hashtags', []))
        return [{'tag': tag, 'count': count} for tag, count in hashtags.most_common(10)]
    
    def _identify_hot_topics(self, messages: List[Dict]) -> List[str]:
        """è¯†åˆ«çƒ­é—¨è¯é¢˜"""
        # åŸºäºå…³é”®è¯ç»„åˆè¯†åˆ«è¯é¢˜
        topics = []
        
        sale_msgs = [m for m in messages if m.get('message_type') == 'sale']
        if len(sale_msgs) >= 2:
            topics.append(f"äº¤æ˜“è®¨è®º ({len(sale_msgs)} æ¡)")
        
        event_msgs = [m for m in messages if m.get('message_type') == 'event']
        if len(event_msgs) >= 1:
            topics.append(f"æ´»åŠ¨ä¿¡æ¯ ({len(event_msgs)} æ¡)")
        
        review_msgs = [m for m in messages if m.get('message_type') == 'review']
        if len(review_msgs) >= 1:
            topics.append(f"è¯„æµ‹åˆ†äº« ({len(review_msgs)} æ¡)")
        
        return topics
    
    def _generate_markdown_report(self, report: Dict, date: str):
        """ç”Ÿæˆ Markdown æ ¼å¼æŠ¥å‘Š"""
        md_content = f"""# Kigurumi ç¤¾åŒºç›‘æµ‹æ—¥æŠ¥ - {date}

> ç”Ÿæˆæ—¶é—´: {report['generated_at']}

## ğŸ“Š æ•°æ®æ¦‚è§ˆ

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| ä»Šæ—¥æ–°æ¶ˆæ¯ | {report['summary']['new_messages_today']} |
| æœ¬å‘¨æ€»æ¶ˆæ¯ | {report['summary']['total_messages_week']} |
| æ´»è·ƒæ¥æº | {report['summary']['active_sources']} |
| æ´»è·ƒç”¨æˆ· | {report['summary']['active_users']} |

## ğŸ“ˆ æ´»è·ƒåº¦åˆ†æ

### æ¶ˆæ¯ç±»å‹åˆ†å¸ƒ
"""
        
        for msg_type, count in report['activity']['by_type'].items():
            md_content += f"- {msg_type}: {count}\n"
        
        md_content += "\n### æ´»è·ƒæ¥æº\n"
        for source, count in report['activity']['by_source'].items():
            md_content += f"- {source}: {count}\n"
        
        md_content += "\n## ğŸ”¥ çƒ­é—¨å†…å®¹\n\n### çƒ­é—¨å…³é”®è¯\n"
        for kw, count in list(report['content_analysis']['top_keywords'].items())[:10]:
            md_content += f"- `{kw}`: {count} æ¬¡\n"
        
        md_content += "\n### çƒ­é—¨è¯é¢˜\n"
        for topic in report['content_analysis']['hot_topics']:
            md_content += f"- {topic}\n"
        
        md_content += "\n## ğŸ‘¥ ç”¨æˆ·ç”»åƒ\n"
        for persona_type, users in report['user_personas'].items():
            md_content += f"\n### {persona_type} ({len(users)} äºº)\n"
            for user in users[:5]:
                md_content += f"- {user}\n"
            if len(users) > 5:
                md_content += f"- ... ç­‰å…± {len(users)} äºº\n"
        
        md_content += f"""
## ğŸ’¹ å¸‚åœºè¶‹åŠ¿

- æ•´ä½“æƒ…æ„Ÿå€¾å‘: {report['market_trends']['sentiment']}
- äº¤æ˜“æ´»è·ƒåº¦: {report['market_trends']['sale_activity']} æ¡ç›¸å…³è®¨è®º
- æ´»åŠ¨æåŠ: {report['market_trends']['event_mentions']} æ¬¡

---
*æœ¬æŠ¥å‘Šç”± Kigurumi Market Intelligence Phase 2 ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
"""
        
        md_file = self.reports_dir / f"report_{date}.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        logger.info(f"Markdown report generated: {md_file}")
    
    def run_monitoring_cycle(self, messages: List[Dict]) -> List[MonitoredMessage]:
        """æ‰§è¡Œä¸€è½®ç›‘æµ‹"""
        new_messages = []
        
        for raw_msg in messages:
            msg = self.process_message(
                source=raw_msg.get('source', 'unknown'),
                source_type=raw_msg.get('source_type', 'channel'),
                author=raw_msg.get('author', 'unknown'),
                content=raw_msg.get('content', ''),
                timestamp=raw_msg.get('timestamp', datetime.now().isoformat()),
                media_type=raw_msg.get('media_type')
            )
            
            if msg:
                self.save_message(msg)
                new_messages.append(msg)
                
                # æ£€æµ‹ç«å“æåŠ
                competitors = self.detect_competitors(msg.content)
                for comp in competitors:
                    intel = CompetitorIntel(
                        name=comp['brand'],
                        source=msg.source,
                        mention_date=msg.timestamp[:10],
                        context=comp['context'],
                        product_type=None,
                        price_range=None,
                        sentiment=comp['sentiment'],
                        urls=msg.urls,
                        collected_at=datetime.now().isoformat()
                    )
                    self.save_competitor_intel(intel)
        
        self._save_seen_ids()
        self._save_stats()
        
        logger.info(f"Monitoring cycle complete. New messages: {len(new_messages)}")
        return new_messages


def main():
    """ä¸»å‡½æ•° - Phase 2 ç›‘æµ‹å…¥å£"""
    
    monitor = KigurumiPhase2Monitor()
    
    # ç¤ºä¾‹ï¼šå¤„ç†ä¸€æ‰¹æ¨¡æ‹Ÿæ•°æ®ï¼ˆå®é™…è¿è¡Œæ—¶æ›¿æ¢ä¸ºçœŸå®æ•°æ®ï¼‰
    sample_messages = [
        {
            'source': 'Kigurumi World',
            'source_type': 'channel',
            'author': 'kig_fan_01',
            'content': 'Just received my new Dollkii mask! The quality is amazing and the hadalabo skin looks so natural. #kigurumi #dollkii',
            'timestamp': datetime.now().isoformat(),
            'media_type': 'photo'
        },
        {
            'source': 'ç€ãã‚‹ã¿æƒ…å ±å±€',
            'source_type': 'channel',
            'author': 'jp_editor',
            'content': 'ä»Šé€±æœ«ã®ã‚³ãƒŸã‚±ã§ç€ãã‚‹ã¿å±•ç¤ºãŒã‚ã‚Šã¾ã™ã€‚NFD Studioã®æ–°ä½œã‚‚å±•ç¤ºã•ã‚Œã‚‹äºˆå®šã§ã™ã€‚ãœã²ãŠè¶Šã—ãã ã•ã„ï¼',
            'timestamp': datetime.now().isoformat(),
        },
        {
            'source': 'KIG å¤´å£³äº¤æµ',
            'source_type': 'group',
            'author': 'user_cn_123',
            'content': 'æœ‰äººçŸ¥é“KigLandçš„å®šåˆ¶ä»·æ ¼å—ï¼Ÿæƒ³åšä¸€ä¸ªèŒç³»è§’è‰²çš„å¤´å£³ï¼Œé¢„ç®—å¤§æ¦‚8000-12000ï¼Œæ±‚æ¨èé è°±çš„å·¥ä½œå®¤',
            'timestamp': datetime.now().isoformat(),
        },
        {
            'source': 'Kigurumi Fan Club',
            'source_type': 'group',
            'author': 'kig_collector',
            'content': 'Selling my Niya kigurumi set, barely used. Includes mask, bodysuit and accessories. DM for price. #forsale #kigurumi',
            'timestamp': datetime.now().isoformat(),
        }
    ]
    
    # æ‰§è¡Œç›‘æµ‹
    new_msgs = monitor.run_monitoring_cycle(sample_messages)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = monitor.generate_daily_report()
    
    print("\n" + "="*60)
    print("Kigurumi Phase 2 Monitor - Run Complete")
    print("="*60)
    print(f"New messages collected: {len(new_msgs)}")
    print(f"Total messages in database: {monitor.stats['total_collected']}")
    print(f"Daily report saved to: {monitor.reports_dir}/report_{datetime.now().strftime('%Y-%m-%d')}.md")
    print(f"\nTop keywords: {list(report['content_analysis']['top_keywords'].keys())[:5]}")
    print(f"User personas identified: {sum(len(v) for v in report['user_personas'].values())} users")
    
    return monitor, report


if __name__ == "__main__":
    monitor, report = main()
