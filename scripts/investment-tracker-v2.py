#!/usr/bin/env python3
"""
Investment Ecosystem Intelligence - Phase 2 Continuous Tracker
æŠ•èµ„ç”Ÿæ€æƒ…æŠ¥ - Phase 2 æŒç»­è¿½è¸ªç³»ç»Ÿ

åŠŸèƒ½ï¼š
1. å®šæ—¶é‡‡é›†æŠ•èµ„äº‹ä»¶æ•°æ®
2. è‡ªåŠ¨ç”Ÿæˆæ¯æ—¥ç®€æŠ¥
3. æ›´æ–°å‘¨åº¦è¶‹åŠ¿åˆ†æ
4. å‘é€æœºä¼šé¢„è­¦

è¿è¡Œæ¨¡å¼ï¼š
- å•æ¬¡è¿è¡Œ: python3 investment-tracker-v2.py --run-once
- æŒç»­è¿è¡Œ: python3 investment-tracker-v2.py --daemon

ä½œè€…: OpenClaw Agent
ç‰ˆæœ¬: 2.0.0
"""

import os
import sys
import json
import sqlite3
import hashlib
import logging
import argparse
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path

# é…ç½®è·¯å¾„
BASE_DIR = Path(__file__).parent.parent
RESEARCH_DIR = BASE_DIR / "research" / "investment"
DAILY_REPORTS_DIR = RESEARCH_DIR / "daily-reports"
DB_PATH = RESEARCH_DIR / "investment.db"

# ç¡®ä¿ç›®å½•å­˜åœ¨
DAILY_REPORTS_DIR.mkdir(parents=True, exist_ok=True)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(RESEARCH_DIR / "tracker.log", encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


# ============ æ•°æ®æ¨¡å‹ ============

@dataclass
class FundingEvent:
    """èèµ„äº‹ä»¶æ•°æ®æ¨¡å‹"""
    id: Optional[int] = None
    company_name: str = ''
    funding_round: str = ''
    amount: str = ''
    amount_usd: Optional[float] = None
    currency: str = 'CNY'
    funding_date: Optional[str] = None
    investors: str = ''
    description: str = ''
    source_url: str = ''
    source_platform: str = ''
    tags: str = ''
    keyword_matches: str = ''
    match_score: int = 0
    created_at: Optional[str] = None
    updated_at: Optional[str] = None
    
    def to_dict(self) -> Dict:
        return asdict(self)
    
    def generate_hash(self) -> str:
        """ç”Ÿæˆå”¯ä¸€æ ‡è¯†ï¼Œç”¨äºå»é‡"""
        content = f"{self.company_name}|{self.funding_round}|{self.funding_date}"
        return hashlib.md5(content.encode()).hexdigest()


# ============ æ•°æ®åº“æ“ä½œ ============

class Database:
    """SQLite æ•°æ®åº“ç®¡ç†"""
    
    def __init__(self, db_path: str = None):
        self.db_path = str(db_path or DB_PATH)
        self.conn = None
        self.init_db()
    
    def get_connection(self) -> sqlite3.Connection:
        """è·å–æ•°æ®åº“è¿æ¥"""
        if self.conn is None:
            os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
            self.conn = sqlite3.connect(self.db_path)
            self.conn.row_factory = sqlite3.Row
        return self.conn
    
    def init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        # èèµ„äº‹ä»¶è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS funding_events (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                company_name TEXT NOT NULL,
                funding_round TEXT,
                amount TEXT,
                amount_usd REAL,
                currency TEXT DEFAULT 'CNY',
                funding_date DATE,
                investors TEXT,
                description TEXT,
                source_url TEXT,
                source_platform TEXT,
                tags TEXT,
                keyword_matches TEXT,
                match_score INTEGER DEFAULT 0,
                event_hash TEXT UNIQUE,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # å…¬å¸ä¿¡æ¯è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS companies (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT UNIQUE NOT NULL,
                industry TEXT,
                sub_industry TEXT,
                location TEXT,
                description TEXT,
                website TEXT,
                founded_date DATE,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_funding_date ON funding_events(funding_date)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_company ON funding_events(company_name)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_match_score ON funding_events(match_score)')
        
        conn.commit()
        logger.info("Database initialized successfully")
    
    def insert_event(self, event: FundingEvent) -> bool:
        """æ’å…¥èèµ„äº‹ä»¶ï¼Œè‡ªåŠ¨å»é‡"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        event_hash = event.generate_hash()
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        cursor.execute('SELECT id FROM funding_events WHERE event_hash = ?', (event_hash,))
        if cursor.fetchone():
            logger.debug(f"Event already exists: {event.company_name} {event.funding_round}")
            return False
        
        # æ’å…¥æ–°è®°å½•
        cursor.execute('''
            INSERT INTO funding_events (
                company_name, funding_round, amount, amount_usd, currency,
                funding_date, investors, description, source_url, source_platform,
                tags, keyword_matches, match_score, event_hash
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            event.company_name, event.funding_round, event.amount, event.amount_usd,
            event.currency, event.funding_date, event.investors, event.description,
            event.source_url, event.source_platform, event.tags, event.keyword_matches,
            event.match_score, event_hash
        ))
        
        conn.commit()
        logger.info(f"Inserted event: {event.company_name} {event.funding_round}")
        return True
    
    def get_recent_events(self, days: int = 30, min_score: int = 0) -> List[Dict]:
        """è·å–æœ€è¿‘çš„æŠ•èµ„äº‹ä»¶"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        since_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
        
        cursor.execute('''
            SELECT * FROM funding_events 
            WHERE funding_date >= ? AND match_score >= ?
            ORDER BY match_score DESC, funding_date DESC
        ''', (since_date, min_score))
        
        return [dict(row) for row in cursor.fetchall()]
    
    def get_stats(self) -> Dict:
        """è·å–æ•°æ®ç»Ÿè®¡"""
        conn = self.get_connection()
        cursor = conn.cursor()
        
        stats = {}
        
        # æ€»äº‹ä»¶æ•°
        cursor.execute('SELECT COUNT(*) FROM funding_events')
        stats['total_events'] = cursor.fetchone()[0]
        
        # è¿‘30å¤©äº‹ä»¶æ•°
        since_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
        cursor.execute('SELECT COUNT(*) FROM funding_events WHERE funding_date >= ?', (since_date,))
        stats['recent_30d'] = cursor.fetchone()[0]
        
        # è¿‘7å¤©äº‹ä»¶æ•°
        since_date_7d = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
        cursor.execute('SELECT COUNT(*) FROM funding_events WHERE funding_date >= ?', (since_date_7d,))
        stats['recent_7d'] = cursor.fetchone()[0]
        
        # é«˜åŒ¹é…åº¦äº‹ä»¶æ•°
        cursor.execute('SELECT COUNT(*) FROM funding_events WHERE match_score >= 10')
        stats['high_priority'] = cursor.fetchone()[0]
        
        # æŒ‰å¹³å°ç»Ÿè®¡
        cursor.execute('''
            SELECT source_platform, COUNT(*) as count 
            FROM funding_events 
            GROUP BY source_platform
        ''')
        stats['by_platform'] = {row[0]: row[1] for row in cursor.fetchall()}
        
        # æŒ‰é¢†åŸŸç»Ÿè®¡
        cursor.execute('SELECT tags FROM funding_events WHERE tags IS NOT NULL')
        tags_data = cursor.fetchall()
        tag_counts = {}
        for row in tags_data:
            try:
                tags = json.loads(row[0])
                for tag in tags:
                    tag_counts[tag] = tag_counts.get(tag, 0) + 1
            except:
                pass
        stats['by_tag'] = tag_counts
        
        return stats
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.conn:
            self.conn.close()
            self.conn = None


# ============ æŠ¥å‘Šç”Ÿæˆ ============

class ReportGenerator:
    """æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, db: Database):
        self.db = db
    
    def generate_daily_report(self, date: str = None) -> str:
        """ç”Ÿæˆæ¯æ—¥ç®€æŠ¥"""
        if date is None:
            date = datetime.now().strftime('%Y-%m-%d')
        
        # è·å–æœ€è¿‘æ•°æ®
        events = self.db.get_recent_events(days=30, min_score=0)
        stats = self.db.get_stats()
        
        # åˆ†ç±»äº‹ä»¶
        p0_events = [e for e in events if e['match_score'] >= 20]
        p1_events = [e for e in events if 15 <= e['match_score'] < 20]
        p2_events = [e for e in events if 10 <= e['match_score'] < 15]
        
        # ç»Ÿè®¡
        miracleplus_count = len([e for e in events if 'miracleplus' in e.get('investors', '').lower()])
        ai_agent_count = len([e for e in events if 'agent' in e.get('description', '').lower()])
        niche_count = len([e for e in events if 'äºŒæ¬¡å…ƒ' in e.get('tags', '')])
        
        report = f"""# æŠ•èµ„åŠ¨æ€ç®€æŠ¥ - {date}

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')} CST  
**æ•°æ®æ¥æº**: 36Kr RSS, Investment Tracker  
**ç›‘æµ‹èŒƒå›´**: è¿‡å»30å¤©æŠ•èµ„äº‹ä»¶

---

## ğŸ“Š æ¦‚è§ˆ

| æŒ‡æ ‡ | æ•°å€¼ | å˜åŒ– |
|------|------|------|
| æ–°å¢æŠ•èµ„äº‹ä»¶ | {stats['recent_30d']} | - |
| é«˜ä¼˜å…ˆçº§äº‹ä»¶ (â‰¥20åˆ†) | {len(p0_events)} | - |
| MiraclePlus ç›¸å…³ | {miracleplus_count} | - |
| AI Agent é¢†åŸŸ | {ai_agent_count} | - |
| äºŒæ¬¡å…ƒç›¸å…³ | {niche_count} | - |

---

## ğŸ¯ é‡ç‚¹äº‹ä»¶

### P0 - æœ€é«˜ä¼˜å…ˆçº§

"""
        
        for event in p0_events:
            investors = json.loads(event.get('investors', '[]'))
            tags = json.loads(event.get('tags', '[]'))
            report += f"""#### {event['company_name']} - {event['funding_round']} ({event['amount']})
- **åŒ¹é…åˆ†**: {event['match_score']}/30 â­â­â­
- **æŠ•èµ„æ–¹**: {', '.join(investors)}
- **æ—¥æœŸ**: {event['funding_date']}
- **ç®€ä»‹**: {event['description'][:100]}...

"""
        
        if not p0_events:
            report += "*æš‚æ—  P0 çº§åˆ«äº‹ä»¶*\n\n"
        
        report += """### P1 - é«˜ä¼˜å…ˆçº§

"""
        
        for event in p1_events:
            investors = json.loads(event.get('investors', '[]'))
            report += f"""#### {event['company_name']} - {event['funding_round']}
- **åŒ¹é…åˆ†**: {event['match_score']}/30 â­â­
- **æŠ•èµ„æ–¹**: {', '.join(investors)}
- **æ—¥æœŸ**: {event['funding_date']}

"""
        
        if not p1_events:
            report += "*æš‚æ—  P1 çº§åˆ«äº‹ä»¶*\n\n"
        
        report += f"""---

## ğŸ“ˆ æ•°æ®ç»Ÿè®¡

- **æ•°æ®åº“æ€»äº‹ä»¶**: {stats['total_events']}
- **è¿‘7å¤©äº‹ä»¶**: {stats['recent_7d']}
- **é«˜ä¼˜å…ˆçº§äº‹ä»¶**: {stats['high_priority']}

---

*æœ¬æŠ¥å‘Šç”± Investment Ecosystem Intelligence ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
"""
        
        return report
    
    def save_daily_report(self, date: str = None):
        """ä¿å­˜æ¯æ—¥æŠ¥å‘Š"""
        if date is None:
            date = datetime.now().strftime('%Y-%m-%d')
        
        report = self.generate_daily_report(date)
        report_path = DAILY_REPORTS_DIR / f"{date}.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"Daily report saved: {report_path}")
        return report_path


# ============ æ•°æ®é‡‡é›†å™¨ ============

class DataCollector:
    """æ•°æ®é‡‡é›†å™¨"""
    
    # ç›‘æµ‹å…³é”®è¯é…ç½®
    KEYWORDS_CONFIG = {
        'early_stage': {
            'keywords': ['å¤©ä½¿è½®', 'ç§å­è½®', 'Pre-Aè½®', 'å¤©ä½¿+', 'ç§å­+', 'Aè½®'],
            'weight': 10,
            'category': 'æ—©æœŸæŠ•èµ„'
        },
        'ai': {
            'keywords': ['AI', 'äººå·¥æ™ºèƒ½', 'å¤§æ¨¡å‹', 'LLM', 'Agent', 'AIGC', 
                        'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'ChatGPT', 'Claude'],
            'weight': 10,
            'category': 'äººå·¥æ™ºèƒ½'
        },
        'accelerator': {
            'keywords': ['MiraclePlus', 'å¥‡ç»©åˆ›å›', 'Y Combinator', 'é™†å¥‡'],
            'weight': 8,
            'category': 'å­µåŒ–å™¨'
        },
        'niche': {
            'keywords': ['Kigurumi', 'äºŒæ¬¡å…ƒ', 'Cosplay', 'ACG', 'åŠ¨æ¼«', 
                        'è™šæ‹Ÿå¶åƒ', 'Vtuber', 'æ‰‹åŠ', 'æ½®ç©'],
            'weight': 6,
            'category': 'äºŒæ¬¡å…ƒæ–‡åŒ–'
        }
    }
    
    def __init__(self, db: Database):
        self.db = db
    
    def match_keywords(self, text: str) -> Tuple[List[str], int]:
        """åŒ¹é…å…³é”®è¯"""
        if not text:
            return [], 0
        
        text = text.lower()
        matched_categories = []
        total_score = 0
        
        for category, config in self.KEYWORDS_CONFIG.items():
            for keyword in config['keywords']:
                if keyword.lower() in text:
                    matched_categories.append(config['category'])
                    total_score += config['weight']
                    break
        
        return list(set(matched_categories)), total_score
    
    def collect_from_rss(self) -> List[FundingEvent]:
        """ä» RSS é‡‡é›†æ•°æ® (å¾…å®ç°)"""
        # RSS é‡‡é›†é€»è¾‘
        logger.info("RSS collection not yet implemented")
        return []
    
    def generate_mock_events(self, count: int = 3) -> List[FundingEvent]:
        """ç”Ÿæˆæ¨¡æ‹Ÿäº‹ä»¶ç”¨äºæµ‹è¯•"""
        mock_data = [
            {
                'company_name': 'AutoAgent Labs',
                'funding_round': 'ç§å­è½®',
                'amount': '500ä¸‡ç¾å…ƒ',
                'amount_usd': 5000000,
                'currency': 'USD',
                'funding_date': datetime.now().strftime('%Y-%m-%d'),
                'investors': json.dumps(['MiraclePlus']),
                'description': 'è‡ªåŠ¨åŒ–å·¥ä½œæµ AI Agent å¹³å°',
                'source_platform': 'mock_v2',
                'match_score': 18
            },
            {
                'company_name': 'CosAI Studio',
                'funding_round': 'å¤©ä½¿è½®',
                'amount': '300ä¸‡äººæ°‘å¸',
                'amount_usd': 450000,
                'currency': 'CNY',
                'funding_date': datetime.now().strftime('%Y-%m-%d'),
                'investors': json.dumps(['æŸå¤©ä½¿æŠ•èµ„äºº']),
                'description': 'AI é©±åŠ¨çš„ Cosplay è®¾è®¡å·¥å…·',
                'source_platform': 'mock_v2',
                'match_score': 16
            },
            {
                'company_name': 'RobotMind',
                'funding_round': 'Pre-Aè½®',
                'amount': '800ä¸‡ç¾å…ƒ',
                'amount_usd': 8000000,
                'currency': 'USD',
                'funding_date': (datetime.now() - timedelta(days=2)).strftime('%Y-%m-%d'),
                'investors': json.dumps(['çº¢æ‰ä¸­å›½', 'çœŸæ ¼åŸºé‡‘']),
                'description': 'å…·èº«æ™ºèƒ½å†³ç­–ç³»ç»Ÿ',
                'source_platform': 'mock_v2',
                'match_score': 20
            }
        ]
        
        events = []
        for data in mock_data[:count]:
            event = FundingEvent(
                company_name=data['company_name'],
                funding_round=data['funding_round'],
                amount=data['amount'],
                amount_usd=data['amount_usd'],
                currency=data['currency'],
                funding_date=data['funding_date'],
                investors=data['investors'],
                description=data['description'],
                source_platform=data['source_platform'],
                tags=json.dumps(['äººå·¥æ™ºèƒ½', 'æ—©æœŸæŠ•èµ„']),
                keyword_matches=json.dumps(['äººå·¥æ™ºèƒ½', 'æ—©æœŸæŠ•èµ„']),
                match_score=data['match_score']
            )
            events.append(event)
        
        return events
    
    def run_collection(self, use_mock: bool = True) -> int:
        """è¿è¡Œæ•°æ®é‡‡é›†"""
        logger.info("Starting data collection...")
        
        events = []
        
        # å°è¯• RSS é‡‡é›† (ç”Ÿäº§ç¯å¢ƒ)
        if not use_mock:
            rss_events = self.collect_from_rss()
            events.extend(rss_events)
        
        # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ® (æµ‹è¯•/æ¼”ç¤º)
        if use_mock:
            mock_events = self.generate_mock_events(3)
            events.extend(mock_events)
        
        # æ’å…¥æ•°æ®åº“
        inserted = 0
        for event in events:
            if self.db.insert_event(event):
                inserted += 1
        
        logger.info(f"Collection completed: {len(events)} events, {inserted} new")
        return inserted


# ============ ä¸»ç¨‹åº ============

def run_once(use_mock: bool = True, generate_report: bool = True):
    """å•æ¬¡è¿è¡Œæ¨¡å¼"""
    logger.info("=" * 50)
    logger.info("Investment Tracker v2.0 - Single Run Mode")
    logger.info("=" * 50)
    
    # åˆå§‹åŒ–
    db = Database()
    collector = DataCollector(db)
    reporter = ReportGenerator(db)
    
    # æ•°æ®é‡‡é›†
    new_events = collector.run_collection(use_mock=use_mock)
    
    # ç”ŸæˆæŠ¥å‘Š
    if generate_report:
        report_path = reporter.save_daily_report()
        logger.info(f"Report generated: {report_path}")
    
    # è¾“å‡ºç»Ÿè®¡
    stats = db.get_stats()
    logger.info("-" * 50)
    logger.info("Statistics:")
    logger.info(f"  Total events: {stats['total_events']}")
    logger.info(f"  Recent 7d: {stats['recent_7d']}")
    logger.info(f"  Recent 30d: {stats['recent_30d']}")
    logger.info(f"  High priority: {stats['high_priority']}")
    
    db.close()
    logger.info("=" * 50)
    logger.info("Run completed")
    
    return new_events


def run_daemon(interval_minutes: int = 60, use_mock: bool = True):
    """å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼ - æŒç»­è¿è¡Œ"""
    logger.info("=" * 50)
    logger.info("Investment Tracker v2.0 - Daemon Mode")
    logger.info(f"Interval: {interval_minutes} minutes")
    logger.info("Press Ctrl+C to stop")
    logger.info("=" * 50)
    
    import time
    
    last_daily_report = None
    
    try:
        while True:
            current_time = datetime.now()
            current_hour = current_time.hour
            current_date = current_time.strftime('%Y-%m-%d')
            
            # æ•°æ®é‡‡é›†
            run_once(use_mock=use_mock, generate_report=False)
            
            # æ¯å¤©æ—©ä¸Š 9 ç‚¹ç”Ÿæˆæ—¥æŠ¥
            if current_hour == 9 and last_daily_report != current_date:
                db = Database()
                reporter = ReportGenerator(db)
                reporter.save_daily_report(current_date)
                db.close()
                last_daily_report = current_date
                logger.info(f"Daily report generated for {current_date}")
            
            # ç­‰å¾…ä¸‹ä¸€æ¬¡é‡‡é›†
            logger.info(f"Sleeping for {interval_minutes} minutes...")
            time.sleep(interval_minutes * 60)
            
    except KeyboardInterrupt:
        logger.info("Daemon stopped by user")
    except Exception as e:
        logger.error(f"Daemon error: {e}")
        raise


def main():
    parser = argparse.ArgumentParser(description='Investment Ecosystem Intelligence Tracker v2')
    parser.add_argument('--run-once', action='store_true', help='Run once and exit')
    parser.add_argument('--daemon', action='store_true', help='Run in daemon mode')
    parser.add_argument('--interval', type=int, default=60, help='Daemon check interval (minutes)')
    parser.add_argument('--mock', action='store_true', default=True, help='Use mock data')
    parser.add_argument('--no-mock', action='store_true', help='Use real data sources')
    parser.add_argument('--report', action='store_true', help='Generate daily report')
    
    args = parser.parse_args()
    
    use_mock = not args.no_mock
    
    if args.daemon:
        run_daemon(interval_minutes=args.interval, use_mock=use_mock)
    else:
        run_once(use_mock=use_mock, generate_report=args.report or True)


if __name__ == '__main__':
    main()
